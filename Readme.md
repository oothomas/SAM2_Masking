# ğŸ¬Â SAMâ€‘2Â /Â SAMURAI Videoâ€‘Masking Pipeline
_A zeroâ€‘shot CLI that converts raw video into photogrammetryâ€‘ready image sets_

---

## 1â€‚Why does this exist?
Building a 3â€‘D model from **video** usually starts with a slog:

1. Extract every frame  
2. Mask each frame by hand  
3. Rename files and inject camera EXIF so WebODMâ€¯/â€¯RealityCapture will accept them  

This repo turns that entire workflow into **one command** by wrapping
MetaÂ AIâ€™s stateâ€‘ofâ€‘theâ€‘art video segmenter **SAMURAIÂ (SAMâ€‘2)** with a lean Python CLI.

> **Result**  
> Draw _one_ box â†’ receive three tidy folders:
>
> ```
> original/   # untouched RGB JPEGs
> masked/     # backgroundâ€‘removed RGB + *_mask.jpg (binary)
> keyframes/  # optional, if you enable keyâ€‘frame extraction
> ```

---

## 2â€‚What are SAMâ€‘2 and SAMURAI?
| Component | Role | Links |
|-----------|------|-------|
| **SAMâ€‘2** | Extends Segmentâ€‘Anything from images to spatioâ€‘temporal video masks | <https://github.com/facebookresearch/sam2> |
| **SAMURAI** | Official implementationÂ + pretrained weights built on SAMâ€‘2 | <https://github.com/yangchris11/samurai> |

The script `sam2/setup_samurai.sh` clones and installs the SAMURAI repo, then copies your chosen checkpoint under `checkpoints/`. This repo already vendors the `sam2/` Python package that your imports rely onâ€”so after running the setup script you donâ€™t need to juggle two separate check-outs.

---

## 3â€‚Key features
* **Codecâ€‘agnostic input** â€“ autoâ€‘converts `.mov`Â â†’Â `.mp4` if needed  
* **Singleâ€‘click ROI** â€“ draw a bounding box on the first frame, then relax  
* **Zeroâ€‘shot multiâ€‘object tracking** â€“ SAMURAI propagates masks frameâ€‘byâ€‘frame  
* **Photogrammetryâ€‘friendly export**
  * `original/` â€“ raw JPEGs with focal length, 35â€¯mmÂ eq., make/model in EXIF  
  * `masked/`Â Â â€“ backgroundâ€‘removed JPEG +Â binary mask  
* **Optional keyâ€‘frame pickers** â€“ Laplacian variance or ORB matcher  
* **Pure CLI** â€“ no Jupyter dependencies  
* **CUDAâ€‘accelerated** â€“ floatâ€‘16 inference for fast GPUs (falls back to CPU)

---

## 4â€‚Quick start

```bash
# 1Â Clone this repo
git clone https://github.com/<you>/sam2-masking-pipeline.git
cd sam2-masking-pipeline

# 2Â Bootstrap SAMURAI once after cloning
#    Installs the SAMURAI repo and copies the chosen weight into checkpoints/
bash sam2/setup_samurai.sh large         # tiny|small|base_plus|large

# 3Â Create an isolated Python env (3.9Â â€“Â 3.11)
conda create -n sam2 python=3.10 -y
conda activate sam2
# 4Â Install PyTorch built for your CUDA version (example: CUDA 12.1)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# 5Â Other Python deps
pip install -r requirements.txt

# 6Â Run ğŸ‰
python -m sam2_masking \
       --video /path/to/MyScan.mov \
       --checkpoint checkpoints/sam2.1_hiera_large.pt \
       --device cuda:0
```

A window appearsâ€”draw a tight box around the object.  
When the progress bar finishes you will find:

```
MyScan/
 â”œâ”€ original/
 â”œâ”€ masked/
 â””â”€ keyframes/   # only if you enable keyâ€‘frame extraction
```

---

## 5â€‚Detailed installation notes

| Item | Linux / macOS | Windows |
|------|---------------|---------|
| **System packages** | `sudo apt install ffmpeg mediainfo libgl1` | `choco install ffmpeg mediainfo` |
| **GPU drivers** | NVIDIAÂ 535â€¯+ for CUDAÂ 12.1 wheels | same |
| **Python** | 3.9Â â€“Â 3.11 via conda/pyenv | Miniconda recommended |

> **CPUâ€‘only?**Â Install the CPU wheels:  
> `pip install torch==2.5.* torchvision==0.20.*` and run with `--device cpu`.

---

## 6â€‚CLI reference

```
python -m sam2_masking --video <file> [options]

Required
  --video PATH            .mov / .mp4 file

Common options
  --checkpoint PATH       Path to a SAMURAI weight file
  --device cuda:0|cpu     Compute device
  --no-convert            Skip the automatic MOVâ†’MP4 step

For the full list:
  python -m sam2_masking --help
```

---

## 7â€‚Project structure

```
sam2_masking/           # installable Python package
  â”œâ”€ core.py            # processing logic
  â”œâ”€ cli.py             # CLI helpers
  â””â”€ __main__.py        # entry point for `python -m sam2_masking`

sam2/                   # SAMURAI repository cloned by setup_samurai.sh
                        # (includes the sam2 package)
checkpoints/            # large .pt weight files â€“ ignored by git
docs/                   # demo GIFs / screenshots (optional)
```

---

## 8â€‚Extending the pipeline
* **Multiple objects** â€“ call `predictor.add_new_points_or_box()` per instance.  
* **Keyâ€‘frames** â€“ import `laplacian_stats` or `detect_keyframes`
  from `sam2_masking.core` and bolt them into your script.  
* **Fineâ€‘tuning** â€“ drop in your own checkpoint; config is inferred from
  the filename suffix (`*_large.pt`, `*_base_plus.pt`, etc.).

---

## 9â€‚FAQ

| Question | Answer |
|----------|--------|
| Does audio survive the MOVâ†’MP4 step? | Noâ€”OpenCV writes videoâ€‘only. |
| How big is the â€œlargeâ€ checkpoint? | â‰ˆâ€¯2â€¯GB (base â‰ˆâ€¯600â€¯MB, tiny â‰ˆâ€¯250â€¯MB). |
| RTXÂ 20â€‘series support? | Yes; FP16 needs computeÂ 7.0+. Otherwise use `--device cpu`. |
| Official Meta support? | Noâ€”community wrapper. SAMâ€‘2Â &Â SAMURAI are Â©Â MetaÂ AI. |

---

## 10â€‚Cite SAM2 and SAMURAI

If this tool aids your research, please cite SAMâ€‘2Â /Â SAMURAI:

```bibtex
@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}

@misc{yang2024samurai,
  title={SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory}, 
  author={Cheng-Yen Yang and Hsiang-Wei Huang and Wenhao Chai and Zhongyu Jiang and Jenq-Neng Hwang},
  year={2024},
  eprint={2411.11922},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2411.11922}, 
}
```

---

## 11â€‚License
* **Wrapper code:** MIT  
* **SAMURAI (sam2/) source:** ApacheÂ 2.0  
* **Model checkpoints:** see SAMURAI licence

Enjoy painless video masking! ğŸš€
